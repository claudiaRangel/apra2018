---
title: "Predicting Volunteers Using Decision Trees - Supervised Learning - classification and ensembles "
output: html_document
---
####Factors: Age, RFM, Lifetime Raised, Number of degrees, Years since Grad, Gender, Alum, Inclination, Number of Events Attended (not as a volunteer)
####Target: Volunteer y/n
####Models attempted: Class, Treebag, Random Forest, SVM
####In all models, the full dataset was used, EXCEPT for the SVM model (because it took too long to fit the parameters). For SVM, a subset of 10000 was used. For each model, we go through the process of fitting the model and predicting using the data as-is and evaluating the performance, then using SMOTE on the data training set and re-fitting, predicting, and re-evaluating the performance. Finally, we use cross-validation on the best method to make sure it's not overfitting the sample, and then export the predicted volunteers (who are not already volunteers).

**Reading and Normalizing numeric data**
```{r}

volunteers<-read.csv("W:/BI and Analytics/Data Analytics/DataSets/R/APRA2018/Volunteers.csv", quote = "", row.names = NULL)

numeric<-c("Age", "RFM", "LifetimeDonation", "Degrees" ,"YearsSinceGrad", "Male" ,"Alum", "InclinationScore", "EventsAttended")
volunteers[,numeric]<-sapply(volunteers[,numeric], as.numeric)

```
#CART
###A classification tree creates a model that predicts whether a person is a volunteer based on our input variables. 

**We read the data and view the proportions of 1's and 0's in the data. The proportion of Volunteers (1's) is 4% ( <15%, which is typically considered a rare event.)**
```{r}
print(table(volunteers$Target))
print(prop.table(table(volunteers$Target)))

```


**Splitting the training and testing datasets**

```{r}
library(caret) 

set.seed(1234)
splitIndex<-createDataPartition(volunteers$Target, p = .8,
                                                  list = FALSE,
                                                  times = 1)

voltrainSplit <- volunteers[ splitIndex,]
voltestSplit <- volunteers[-splitIndex,]
voltrainSplit$Target <- as.factor(voltrainSplit$Target)
voltestSplit$Target <- as.factor(voltestSplit$Target)
```

**We fit the model and see what it looks like.**
```{r}
library(rpart)
clModel<-rpart(Target ~ Age + RFM + LifetimeDonation +  Degrees + YearsSinceGrad  + Male  + Alum +  InclinationScore  + EventsAttended,
            data = voltrainSplit,
           method="class")

library(rattle)
fancyRpartPlot(clModel)
```


**Here we make the predictions.** 
```{r}

predCLASS <- predict(clModel, newdata = voltestSplit,type = "class", na.action = na.pass)

```
**And show the confusion matrix (no SMOTE, model = class).** 
```{r}
confusionMatrix(data = predCLASS, voltestSplit$Target, positive = "1")
```

####Because there's such a small % of volunteers in the data, we use Synthetic Minority Oversampling Technique to balance out the data.Then we can see if this more balanced dataset helps the model score better than the rare events dataset. Let's try SMOTE-ing the training data to see if we can improve on model's performance.
```{r}
library(DMwR)
voltrainSplit$Target<-as.factor(voltrainSplit$Target)
voltrainSplit <- SMOTE(Target ~ numeric, voltrainSplit, perc.under = 100)
print(table(voltrainSplit$Target))
print(prop.table(table(voltrainSplit$Target)))

```
**Training the model again, this time on smoted dataset**
```{r}
clModel_SMOTE<-rpart(Target ~ Age + RFM + LifetimeDonation +  Degrees + YearsSinceGrad  + Male  + Alum +  InclinationScore  + EventsAttended,
            data = voltrainSplit,
           method="class")

fancyRpartPlot(clModel_SMOTE)
```
**Predicting on Test data**
```{r}
predCLASS_SMOTE <- predict(clModel_SMOTE, newdata = voltestSplit,type = "class", na.action = na.pass)

```
**Based on the Confusion Matrix (SMOTE, model = class), the ability of the classification model to predict volunteers is better using the SMOTE'd data. Out of the 35 volunteers in the test data, it predicted 32 (instead of 29 before).**
```{r}
confusionMatrix(data = predCLASS_SMOTE, voltestSplit$Target, positive="1")
```

#Random Forest 
###The Random Forest, a supervised learning model, is a decision tree-based mode, with two parameters - the number of decision trees, and the number of features to search over to find the best feature that splits the node. By randomly selecting a feature subset for each split, it improves variance by reducing correlation between trees.The method creates a collection of totally unique trees that all make their classifications differently - and the majority decision is chosen. Each tree grows out fully and overfits, but in different ways, so mistakes each makes are averaged out over all of them.

**Splitting the training and testing datasets**
```{r}
set.seed(1234)
splitIndex<-createDataPartition(volunteers$Target, p = .8,
                                                  list = FALSE,
                                                  times = 1)
voltrainSplit <- volunteers[ splitIndex,]
voltestSplit <- volunteers[-splitIndex,]
voltrainSplit$Target <- as.factor(voltrainSplit$Target)
voltestSplit$Target <- as.factor(voltestSplit$Target)
```
**We tune the model to confirm the best number of mtry, aka the number of obs randomly selected by the model to use to make splitting decisions at each split. The tuner says 2 (the number of mtry that minimizes the out of bag (prediction) error**
```{r}
library(randomForest)
set.seed(1)
res<-tuneRF(x=subset(voltrainSplit, select= -Target), y=voltrainSplit$Target, ntreeTry=500)
```
**Training the model**
```{r}
rfModel<- randomForest(Target ~ Age + RFM + LifetimeDonation +  Degrees + YearsSinceGrad  + Male  + Alum +  InclinationScore  + EventsAttended, data = voltrainSplit, ntree = 500, mtry = 2)
```
**Predicting on Test data**
```{r}
predRF <- predict(rfModel, newdata = voltestSplit)
```
**Confusion Matrix (no SMOTE, model = Random Forest)**
```{r}
confusionMatrix(data = predRF, voltestSplit$Target, positive="1")
```
**The model's performance in identifying volunteers is pretty good but let's try SMOTE-ing the training data to get it better trained to sort 1's.**
```{r}
voltrainSplit <- SMOTE(Target ~ numeric, voltrainSplit, perc.under = 100)

```
**Training the model again, this time on smoted dataset**
```{r}
rfModel<- randomForest(Target ~ Age + RFM + LifetimeDonation +  Degrees + YearsSinceGrad  + Male  + Alum +  InclinationScore  + EventsAttended, data = voltrainSplit, ntree = 500, mtry = 3,  keep.forest=TRUE)
```
**Predicting on Test data**
```{r}
predRF_SMOTE <- predict(rfModel, newdata = voltestSplit)
```
**Confusion Matrix (after SMOTE, model = Random Forest) shows improvement in ability to predict volunteers, but due to its increased sensitivity it'll be more likely now to sense volunteers where there aren't any.**

```{r}
confusionMatrix(data = predRF_SMOTE, voltestSplit$Target, positive="1")
```
####Smote-ing the data improves sensitivity to the the prediction of volunteers. SMOTE-ing also lowers slightly the ability to predict non-volunteers, but it was only so high prior to SMOTE-ing because there were so few volunteers, it would almost always be correct in predicting that it wasn't a volunteer!

####Now we address the potential for overfitting the sample.

Overfitting = building a model on training data and it picks up not only relationships between the outcome and predictors, but also noise specific to the training set. Testing on a separate test set can help prevent this, but where the dataset is not very large, this can be a challenging balance - to get better training, you take away from the test set, and to get a more representative test set, you take away from the training. Although our dataset is technically large, our rare-events problem (with very few volunteers) means it's actually a 'small' dataset. 

To get around this, we use k-fold cross validation to get a sense of the out-of-sample accuracy. We create several testing/training sets by splitting the data into k equally sized subsets, and treat a single subsample as the testing set and the remaining data as the training set. We run and test models on all k datasets and average the estimates. After, we take it further and repeat the k-fold CV a large number of times (200) and take the mean of this estimate - this will alow us to get an estimate of the precision of the out-of-sample accuracy by creating a confidence interval.


##We try the k-fold cross validation to predict the efficiency of the method we're using to classify our data

**We create a function, k.folds, that takes a specified colummn containing classifiers (volunteer$Target, in our case) and creates k=10 folds using indices. For this example we test the random forest model**
```{r}

volunteers$Target<-as.factor(volunteers$Target)

k.folds <- function(k) {
    folds <- createFolds(volunteers$Target, k = k, list = TRUE,   returnTrain = TRUE)
    for (i in 1:k) {
        model <- randomForest(Target ~ ., data = volunteers[folds[[i]],], ntree = 500, mtry = 2)
        predictions <- predict(object = model, newdata = volunteers[-folds[[i]],], type = "response")
        accuracies.dt <- c(accuracies.dt, 
                           confusionMatrix(predictions, volunteers[-folds[[i]], ]$Target)$overall[[1]])
    }
    accuracies.dt
}

set.seed(100)
accuracies.dt <- c()
accuracies.dt <- k.folds(10)
accuracies.dt
```

**The mean accuracy is 99%, which is pretty good**
```{r}
mean.accuracies<-mean(accuracies.dt)
mean.accuracies
```
**Prepare and export the predicted volunteers who have not yet actually volunteered**
```{r}
library(dplyr)
predictedvols<-cbind(voltestSplit, predRF_SMOTE)
predictednewvols<-subset(predictedvols, Target == 0 & predRF_SMOTE == 1)
predictednewvols<-predictednewvols %>% select(ID, Target, predRF_SMOTE)
write.csv(predictednewvols,"PredictedVolunteers.csv", row.names = FALSE)
```
---
#####__Data Source__: Internal CRM
#####__Date Prepared__: July 2018
#####__Presentation__: APRA 2018 Data Analytics Symmposium



